package cn.itheima.NetWordCount



import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * 通过SparkStreaming实时的计算客户端传过来的单词出现的次数
  * jar包已经成功 但是出现一个问题 这个只能计算一个批次的单词出现的次数 但是
  * 需求是我们想要计算出每个批次累加的单词出现的次数 该怎么实现呢
  */
object WordCountDemo {
  def main(args: Array[String]): Unit = {
    /**
      * create a streamcontext with one second batch size
      * The parameter are SparkConf and Seconds
      */
    val conf = new SparkConf().setAppName("WordCountDemo").setMaster("spark://dshuju02:7077")
    val sparkstream = new StreamingContext(conf, Seconds(10))
    /**
      * Create a socket stream on target ip:port and count the words
      * in input stream of \n delimited text (eg. generated by 'nc')Note
      * that no duplication in storage level only for running locally.
      * Replication necessary in distributed scenario for fault tolerance.
      */
    val dstream: ReceiverInputDStream[String] = sparkstream.socketTextStream("192.168.2.129",9999)
    val dstream1: DStream[(String, Int)] = dstream.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)

    /**
      * 在以后的工作当中 这个数据存储在redis当中比较多  不会打印在控制台上
      */
    dstream1.print()
    sparkstream.start()
    sparkstream.awaitTermination()
  }
}

/**
  *
  * 计算出每个批次累加的单词出现的次数
  */
object StateWordCountDemo{
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("StateWordCountDemo").setMaster("spark://dshuju02:7077")
    val streamingContext = new StreamingContext(conf,Seconds(5))
    val unit: ReceiverInputDStream[String] = streamingContext.socketTextStream("192.168.2.129",8888)

    /**
      * 我们想要实现将每个批次的数据进行累加 要运用一个方法updateStateByKey() 这个方法需要三个参数
      *
      */
    //unit.flatMap(_.split(" ")).map((_,1)).updateStateByKey()
  }
}

/**
  * 通过监控一个本地文件系统 或者 hdfs 文件系统的某个文件夹 来计算该文件夹下的单词出现的次数
  */
object FileWordCount{
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("spark://dshuju02:7077").setAppName("FileWordCount")
    val streamcontex = new StreamingContext(conf,Seconds(5))
    /**
      * 1:Spark Streaming will monitor the directory dataDirectory
      * and process any files created in that directory (files written in nested directories not supported).
      * Note that
      * 1:The files must have the same data format.
      * 2:The files must be created in the dataDirectory by atomically moving or renaming them into the data directory.
      * 3:Once moved, the files must not be changed. So if the files are being continuously appended, the new data will not be read.
      * 4:For simple text files, there is an easier method streamingContext.textFileStream(dataDirectory). And file streams do not require running a receiver, hence does not require allocating cores
      */
    val inputDStream: DStream[String] = streamcontex.textFileStream("monitor the directory dataDirectory")
    val dStream: DStream[(String, Int)] = inputDStream.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
    dStream.print()
    streamcontex.start()
    streamcontex.awaitTermination()
  }
}

/**
  * 在上述的例子当中 我们只是将计算出的数据打印到控制台上，
  * 但是在实际的工作当中我们应该经计算好的数据存储到数据库当中
  */

object TextToMysqll{
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("spark://dshuju02:7077").setAppName("TextToMysql")
    val streamcontex = new StreamingContext(conf,Seconds(10))
    val dstream: ReceiverInputDStream[String] = streamcontex.socketTextStream("192.168.2.129",8888)
    val unit: DStream[(String, Int)] = dstream.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
    /*unit.foreachRDD( rdd =>{
      rdd.foreachPartition( partition =>{
        if (partition.size>0){
          Class.forName("com.mysql.jdbc.Driver")
          val connection: Connection = DriverManager.getConnection("jdbc:mysql://192.168.2.129:3306/SparkStream","root","123456")
          val sql = "insert into wordcount values(?,?)"
          var statement: PreparedStatement =null
          partition.foreach(x => {
            statement = connection.prepareStatement(sql)
            statement.setString(1,x._1)
            statement.setInt(2,x._2)
            statement.executeUpdate()
          })
          connection.close()
        }
      })
    })*/
    unit.print()
    streamcontex.start()
    streamcontex.awaitTermination()
  }
}
